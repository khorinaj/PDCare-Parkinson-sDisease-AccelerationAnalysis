# Activity Recognition Pipeline

## Description
This project implements a complete machine learning pipeline for activity recognition using a Random Forest classifier. It processes features from publicly available Human Activity Recognition (HAR) datasets, HARTH and HAR70+, and applies machine learning techniques to predict human activities. Additionally, the pipeline extends to make predictions on an external Parkinson’s Disease (PD) dataset.

The pipeline consists of the following stages:
- Data Preprocessing
- Model Training
- Model Evaluation
- Predictions on External Data


## Folder Structure
```plaintext

├── env/                      # Virtual environment (excluded from Git)
├── features/                 # Preprocessed features and labels (will be saved in this folder)
│   ├── features_labels_participants.csv # generated by author 
│   ├── processed_features_matlab.csv # will be generated by this code using matlab and saved here
│   ├── processed_features_python.csv # will be generated by this code using python and saved here
├── Public Data/              # Raw datasets (not included in repository)
│   ├── har70plus/            # HAR70+ dataset files
│   ├── harth/                # HARTH dataset files
├── results/                  # Output results, models, and visualizations
│   ├── CM4classfs36_pt2.png  # confusion matrix of author's trained model
│   ├── CM4classfs36_pt2_percentage.png # confusion matrix of author's trained model (percentage)
│   ├── featureImportance.xlsx # feature importance score generated by author' model
│   ├── my_model.pkl # model that will be generated by this code and saved here
│   ├── my_model4class.pkl # author' trained model
│   ├── my_model4classfs36_pt2.pkl # author' trained model after feature selection
├── src/                      # Source scripts for processing and ML pipeline
│   ├── preprocess_feature_extraction.py
│   ├── preprocess_feature_extraction.m
│   ├── train_model.py
│   ├── model_evaluation.py
│   ├── prediction.py
├── main.py                   # Main pipeline entry point
├── README.md                 # Project documentation
├── requirements.txt          # Python dependencies
```

## HARTH and HAR70+ Datasets
The datasets used in this project are sourced from the [NTNU AI-Lab HARTH repository](https://github.com/ntnu-ai-lab/harth-ml-experiments).
### How to Download and Organize the Datasets
1. Visit the NTNU AI-Lab HARTH repository.
2. Download the HARTH and HAR70+ datasets.
3. Upzip and copy the folder called har70plus/ and harth/ directly in Public data folder (as shown below):
```plaintext
Public Data/
├── har70plus/
│   ├── 50x.csv
│   └── ...
├── harth/
│   ├── S0xx.csv
│   └── ...
```

## Workflow
The pipeline consists of the following steps:

1. **Data Preprocessing**:
   - Extract features from the HAR dataset.
   - Script: `src/preprocess_feature_extraction.py` and `src/preprocess_feature_extraction.main`
   Note: The author used the MATLAB file for feature extraction and training the model. 
   There might have slight differences in features extracted using python and MATLAB due to two different systems. 

2. **Model Training**:
   - Train a Random Forest model using preprocessed features.
   - Script: `src/train_model.py`.

3. **Model Evaluation**:
   - Evaluate the trained model using Leave-One-Participant-Out Cross-Validation (LOPOCV).
   - Generate metrics such as accuracy, precision, recall, and F1-score.
   - Script: `src/model_evaluation.py`.

4. **Prediction**:
   - Use the trained model to predict activity labels on a processed PD dataset.
   - Script: `src/prediction.py`.
   - Dependencies: This need the PD data to be preprocessed and the feature is extracted and saved in 'fPD' variable in ID_processed.mat in the folder: PDMonitor/PDanalysis/dataProcessed_Body

## Outputs
features extracted are saved in features/processed_features_matlab.csv or processed_features_python.csv 
All results about the ML model are saved in the results/ folder:
- Confusion Matrices: Visualizations of model performance.
- Feature Importance Scores: featureImportance.xlsx.
- Trained Models: Serialized .pkl files for reuse.

---
## Setup Instructions
### Prerequisites
To run this project, you will need:

- **Python 3.x**: Ensure Python is installed on your system. You can download it from [python.org](https://www.python.org/).
- **pip**: The Python package manager (comes pre-installed with Python).

### Step 1: Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/activity-recognition-pipeline.git
   ```
### Step 2: Create a Virtual Environment

To ensure all dependencies are installed correctly and avoid conflicts with other projects, it's recommended to use a virtual environment.
1. Navigate to your project directory:
   ```bash
   cd /path/to/your/project
   ```
2. Create a virtual environment:
   ```bash
   python3 -m venv env
   ```
3. Activate the virtual environment:
   ```bash
   source env/bin/activate # FOR Linux/macOS
   .\env\Scripts\activate # Window
   ```
Your terminal prompt should now show (env) at the beginning, indicating the virtual environment is active.

### Step 3: Install Dependencies
1. Install the required libraries using requirements.txt:
   ```bash
   pip install -r requirements.txt
   ```
---

### Step 4: Run project
#### Run the entire pipeline
   ```bash
    python main.py   
   ```
#### Running Individual Steps:
   ```bash
   python src/preprocess_feature_extraction.py
   python src/train_model.py
   python src/model_evaluation.py
   python src/prediction.py
   ```
   
## License
This project is licensed under the MIT License.

## Author
Yanke Sun
Email: khorinaj@outlook.com

## Citation


